{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "T5tSUn3wZFEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/onlookertozip/SamsungElectronics_Gumi.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSh2hcg1ZCZb",
        "outputId": "b8a2621e-9405-4a59-c0c8-863e2621f2d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SamsungElectronics_Gumi'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 77 (delta 23), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (77/77), 26.55 MiB | 7.69 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed0f96f1-910f-438f-876f-9eff119c2b0a"
      },
      "source": [
        "# Part1: Langchain 기초\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a5b1e2f-9d67-4d01-9a8c-83ced6b711a9"
      },
      "source": [
        "## 1. 환경 구성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b03b7854-f96a-47fc-b3c7-b2bdfb55df81"
      },
      "source": [
        "### 1) 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c9ec29f-78ac-49fd-a5c9-e846af7a8ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.6/152.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'langchain-openai' candidate (version 0.1.21 at https://files.pythonhosted.org/packages/e5/0c/7c752ba4a6fa8719f9f8cf8e22f0e2c8bbcdac5f8a580dd12a1496cd5031/langchain_openai-0.1.21-py3-none-any.whl (from https://pypi.org/simple/langchain-openai/) (requires-python:<4.0,>=3.8.1))\n",
            "Reason for being yanked: Regression. AzureChatOpenAI json-mode broken. Fixed in 0.1.22.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.6/934.6 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.20 requires pydantic>=2.7.0, but you have pydantic 1.10.19 which is incompatible.\n",
            "langchain-text-splitters 0.3.2 requires langchain-core<0.4.0,>=0.3.15, but you have langchain-core 0.2.43 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.0.181 langchain_openai==0.1.21 httpx==0.27.2 tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55152049-e9e5-4952-8e19-409f58cf3ac9"
      },
      "source": [
        "### 2) OpenAI 인증키 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b76f68a8-4745-4377-8057-6090b87377d1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc01c50a-32cf-49af-891a-f9b17fa0bd6c"
      },
      "source": [
        "## 2. LLM Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23729d10-9600-415b-b7d1-f954665224e3"
      },
      "source": [
        "### 1) Prompt + LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# model\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "# chain 실행\n",
        "result = llm.invoke(\"지구의 자전 주기는?\")"
      ],
      "metadata": {
        "id": "on0y4xF8VoyE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "WzcZy4PruV1n",
        "outputId": "116ffb72-5d0f-46ae-f5ff-a0d26371e9aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'지구의 자전 주기는 약 24시간입니다. 이는 하루 동안 지구가 자전하는 시간을 나타내며, 이에 따라 낮과 밤이 생기게 됩니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"You are an expert in astronomy. Answer the question. <Question>: {input}\")\n",
        "prompt"
      ],
      "metadata": {
        "id": "SeNi_VXqYD-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296afb57-c58c-4238-dd38-e33ffad87d03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='You are an expert in astronomy. Answer the question. <Question>: {input}'))])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "# chain 연결 (LCEL)\n",
        "chain = prompt | llm\n",
        "\n",
        "# chain 호출\n",
        "chain.invoke({\"input\": \"지구의 자전 주기는?\"})"
      ],
      "metadata": {
        "id": "01WLucSpYjZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7a24edb-60c3-43f5-e48d-a986aab2df0a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='지구의 자전 주기는 약 24시간입니다. 이것은 하루의 길이를 의미하며, 지구가 자전하면서 자전축 주위를 한 바퀴 도는데 약 24시간이 걸리기 때문입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 30, 'total_tokens': 105, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c9746477-e9f8-4095-8298-c45ff30cb7bc-0', usage_metadata={'input_tokens': 30, 'output_tokens': 75, 'total_tokens': 105})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "f42fc0d1-c7a1-48a1-8e26-9718a1443be4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0aba9b0c-ea84-489b-9ff2-dfbe2541baba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'지구의 자전 주기는 약 24시간 혹은 정확히 23시간 56분 4.1초입니다. 이것은 지구가 자전하는 데 걸리는 시간으로, 하루의 길이를 결정짓는 요소 중 하나입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# prompt + model + output parser\n",
        "prompt = ChatPromptTemplate.from_template(\"You are an expert in astronomy. Answer the question. <Question>: {input}\")\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# LCEL chaining\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "# chain 호출\n",
        "chain.invoke({\"input\": \"지구의 자전 주기는?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0188674-915f-41af-ac46-56f9f54289b0"
      },
      "source": [
        "### 2) Multiple Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "31678c55-38a8-4ca2-b437-9d4495946b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f83840f6-4afc-45e1-9793-6d6d3e17f02e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The word \"미래\" translates to \"future\" in English.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "prompt1 = ChatPromptTemplate.from_template(\"translates {korean_word} to English.\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"explain {english_word} using oxford dictionary to me in Korean.\"\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "chain1 = prompt1 | llm | StrOutputParser()\n",
        "\n",
        "chain1.invoke({\"korean_word\":\"미래\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "16718b76-f59d-48f7-906f-5d2371417803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c4c0c166-6e29-4487-bbe4-9b5ecb4c9122"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'미래는 어떤 시간으로부터 보아 예상되는 또는 예견되는 일이나 상황을 가리키는 말이다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "chain2 = (\n",
        "    {\"english_word\": chain1}\n",
        "    | prompt2\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain2.invoke({\"korean_word\":\"미래\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "668cb998-71fb-4fd9-a6f1-61aad022fa3e"
      },
      "source": [
        "## 3. Prompt\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16573676-6ba7-439a-a989-1e7d13420e95"
      },
      "source": [
        "### 1) PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "096f63a1-6b04-4d1d-aaaf-c5a77c5d3ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "21962f79-90a0-4533-a709-9525d06c36ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요, 제 이름은 홍길동이고, 나이는 30살입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# 'name'과 'age'라는 두 개의 변수를 사용하는 프롬프트 템플릿을 정의\n",
        "template_text = \"안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다.\"\n",
        "\n",
        "# PromptTemplate 인스턴스를 생성\n",
        "prompt_template = PromptTemplate.from_template(template_text)\n",
        "\n",
        "# 템플릿에 값을 채워서 프롬프트를 완성\n",
        "filled_prompt = prompt_template.format(name=\"홍길동\", age=30)\n",
        "\n",
        "filled_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4a314a0f-11f3-47f0-aed7-d405b837a273",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c251bae3-1495-453b-f5c5-c3dc628863ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['age', 'language', 'name'], template='안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다.\\n\\n아버지를 아버지라 부를 수 없습니다.\\n\\n{language}로 번역해주세요.')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
        "combined_prompt = (\n",
        "              prompt_template\n",
        "              + PromptTemplate.from_template(\"\\n\\n아버지를 아버지라 부를 수 없습니다.\")\n",
        "              + \"\\n\\n{language}로 번역해주세요.\"\n",
        ")\n",
        "\n",
        "combined_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ca859903-85e4-4630-a173-fffabb24a766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "019a387a-2995-4ae6-e307-fa0586c164d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요, 제 이름은 홍길동이고, 나이는 30살입니다.\\n\\n아버지를 아버지라 부를 수 없습니다.\\n\\n영어로 번역해주세요.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "combined_prompt.format(name=\"홍길동\", age=30, language=\"영어\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "63a96988-196b-4c6f-ac01-6f8c56386af3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0ceaba0a-7144-4cf7-e88e-c0134991179b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, my name is Hong Gil-dong and I am 30 years old.\\n\\nI cannot call my father \"father\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "chain = combined_prompt | llm | StrOutputParser()\n",
        "chain.invoke({\"age\":30, \"language\":\"영어\", \"name\":\"홍길동\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa617bbb-8ccb-4aab-a7df-6cd8232ac92f"
      },
      "source": [
        "### 2) ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "10dc9e58-0555-49d1-9ea0-faba6bd63f55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a409c5af-d919-4f2c-adfe-d7cfb15f8dd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='이 시스템은 천문학 질문에 답변할 수 있습니다.'),\n",
              " HumanMessage(content='태양계에서 가장 큰 행성은 무엇인가요?')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# 2-튜플 형태의 메시지 목록으로 프롬프트 생성 (type, content)\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
        "    (\"user\", \"{user_input}\"),\n",
        "])\n",
        "\n",
        "messages = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f1a8fcae-33bd-43d9-a55e-9f37e82f5484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d1392ca4-0215-4853-833a-5f07559271f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'태양계에서 가장 큰 행성은 목성입니다. 목성은 질량, 부피, 지름 등 여러 측면에서 가장 큰 행성으로 알려져 있습니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "chain = chat_prompt | llm | StrOutputParser()\n",
        "\n",
        "chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e332a435-3379-4b08-8800-2c758eea9eca"
      },
      "source": [
        "### 3) Message"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MessagePromptTemplate 활용\n",
        "\n",
        "from langchain_core.prompts import SystemMessagePromptTemplate,  HumanMessagePromptTemplate\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessagePromptTemplate.from_template(\"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "messages = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1bpbNOyINkM",
        "outputId": "df4f6e08-21b3-40f4-9429-9b31e6bd396c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='이 시스템은 천문학 질문에 답변할 수 있습니다.'),\n",
              " HumanMessage(content='태양계에서 가장 큰 행성은 무엇인가요?')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = chat_prompt | llm | StrOutputParser()\n",
        "\n",
        "chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "oFv2sFWwLH3L",
        "outputId": "449e62ab-0fb1-45ff-971a-9269d8dd409a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'태양계에서 가장 큰 행성은 목성입니다. 목성은 지름이 약 142,984km로 태양계에서 가장 큰 행성이며, 매우 큰 가스 행성입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62f6cbd9-769e-4671-89cc-8fb3da5eca2d"
      },
      "source": [
        "## 5. Model Parameter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 모델 클래스 유형"
      ],
      "metadata": {
        "id": "OrGWildXSaQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LLM"
      ],
      "metadata": {
        "id": "2xwl8LRsSkyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "\n",
        "llm.invoke(\"한국의 대표적인 관광지 3군데를 추천해주세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MPNGvV1ASv4R",
        "outputId": "2c3bfd48-cbcf-4701-b32c-011bbde58b30"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n1. 경복궁\\n2. 남산 타워\\n3. 제주도'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ChatModel"
      ],
      "metadata": {
        "id": "n4p-sPIXSp6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI()\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"이 시스템은 여행 전문가입니다.\"),\n",
        "    (\"user\", \"{user_input}\"),\n",
        "])\n",
        "\n",
        "chain = chat_prompt | chat\n",
        "chain.invoke({\"user_input\": \"안녕하세요? 한국의 대표적인 관광지 3군데를 추천해주세요.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoS6GNWQSwmF",
        "outputId": "42061a7a-adad-42c6-ff42-84ed7bd7f801"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='안녕하세요! 한국의 대표적인 관광지로는 다음 3군데를 추천해드립니다:\\n\\n1. 경복궁 (Gyeongbokgung): 서울에 위치한 경복궁은 조선 시대의 궁궐로, 한국의 역사와 문화를 경험할 수 있는 곳입니다. 경복궁 근처에는 국립고궁박물관과 청와대도 있어서 함께 방문하기 좋습니다.\\n\\n2. 부산 해운대 해변 (Haeundae Beach): 부산의 대표적인 해변으로 유명한 해운대 해변은 아름다운 바다 풍경과 다양한 레스토랑, 상점이 있는 곳입니다. 여름에는 많은 관광객들이 모여 바다에서 즐거운 시간을 보낼 수 있습니다.\\n\\n3. 경주 (Gyeongju): 경주는 한국의 역사적인 도시로, 많은 문화유산과 유적지가 있습니다. 세계문화유산으로 지정된 석굴암이나 첨성대, 불국사 등을 방문하여 한국의 고대 역사를 체험할 수 있습니다.\\n\\n이 외에도 한국에는 다양한 멋진 관광지가 많이 있으니, 방문하고 싶은 지역이 있으시면 더 자세한 정보를 알려주시면 더 많은 도움을 드릴 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 441, 'prompt_tokens': 59, 'total_tokens': 500, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-59b4ae34-caff-4403-b544-6246234b948b-0', usage_metadata={'input_tokens': 59, 'output_tokens': 441, 'total_tokens': 500})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f4f0d21-c236-41dc-906f-e2d58c02b1ea"
      },
      "source": [
        "### 2) 모델 파라미터 설정\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 모델에 직접 파라미터를 전달 (모델 생성 시점)"
      ],
      "metadata": {
        "id": "mkaCYK13oc4-"
      }
    },
    {
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 모델 파라미터 설정\n",
        "params = {\n",
        "    \"temperature\": 0.7,         # 생성된 텍스트의 다양성 조정\n",
        "    \"max_tokens\": 100,          # 생성할 최대 토큰 수\n",
        "    \"frequency_penalty\": 0.5,   # 이미 등장한 단어의 재등장 확률\n",
        "    \"presence_penalty\": 0.5,    # 새로운 단어의 도입을 장려\n",
        "    \"stop\": [\"\\n\"]              # 정지 시퀀스 설정\n",
        "}\n",
        "\n",
        "\n",
        "# 모델 인스턴스를 생성할 때 설정, model_kwargs 제거 후 params에 직접 전달\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", **params)\n",
        "\n",
        "\n",
        "# 모델 호출\n",
        "question = \"태양계에서 가장 큰 행성은 무엇인가요?\"\n",
        "response = model.invoke(input=question)\n",
        "\n",
        "# 전체 응답 출력\n",
        "print(response)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vlv8XDKbF7d",
        "outputId": "f23fb3e5-4e28-4063-c8f5-dbf89e2e4313"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='태양계에서 가장 큰 행성은 목성(Jupiter)입니다. 목성은 태양계 내에서 질량과 부피가 가장 크며, 지름이 또한 가장 큰 행성입니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 29, 'total_tokens': 96, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-db00e633-0fce-43ab-9421-deaf26101900-0' usage_metadata={'input_tokens': 29, 'output_tokens': 67, 'total_tokens': 96}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 모델에 직접 파라미터를 전달 (모델 호출 시점)"
      ],
      "metadata": {
        "id": "yAxlgzozpGMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 파라미터 설정\n",
        "params = {\n",
        "    \"temperature\": 0.7,         # 생성된 텍스트의 다양성 조정\n",
        "    \"max_tokens\": 10,          # 생성할 최대 토큰 수\n",
        "}\n",
        "\n",
        "# 모델 인스턴스를 호출할 때 전달\n",
        "response = model.invoke(input=question, **params)\n",
        "\n",
        "# 문자열 출력\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9nGM_MroWwS",
        "outputId": "c4fc6387-f9dd-42e3-b5fc-2969f8cb2920"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "태양계에서 가장 큰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd39280c-2de0-49bf-8ee9-51362d50b6cb"
      },
      "source": [
        "#### 모델에 추가적인 파라미터를 전달\n",
        "- bind 메서드를 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ec858125-f9ec-447c-ad84-c69f81399e91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f60f0ab-4901-4232-bc9b-c5c338381855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='태양계에서 가장 큰 행성은 목성입니다. 목성은 지름이 약 142,984km로 가장 큰 행성이며, 대기 중 99% 이상이 수소와 헬륨으로 이루어져 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 58, 'total_tokens': 129, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-05ab287d-a58c-47eb-b3ff-a4bd33067916-0' usage_metadata={'input_tokens': 58, 'output_tokens': 71, 'total_tokens': 129}\n",
            "content='태양계에서 가장 큰' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 58, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'length', 'logprobs': None} id='run-f4e54a50-3caf-4300-929f-8acb4750ac2f-0' usage_metadata={'input_tokens': 58, 'output_tokens': 10, 'total_tokens': 68}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
        "    (\"user\", \"{user_input}\"),\n",
        "])\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", max_tokens=100)\n",
        "\n",
        "messages = prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
        "\n",
        "before_answer = model.invoke(messages)\n",
        "\n",
        "# # binding 이전 출력\n",
        "print(before_answer)\n",
        "\n",
        "# 모델 호출 시 추가적인 인수를 전달하기 위해 bind 메서드 사용 (응답의 최대 길이를 10 토큰으로 제한)\n",
        "chain = prompt | model.bind(max_tokens=10)\n",
        "\n",
        "after_answer = chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})\n",
        "\n",
        "# binding 이후 출력\n",
        "print(after_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5578014-f828-45e7-bd15-dde39ea72413"
      },
      "source": [
        "## 6. Output Parsers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aff3bd5-1dc0-4b60-ae36-78336bfa927d"
      },
      "source": [
        "### 1) CSV Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "e5670c36-abbe-4f33-a736-87fbb2d8e72a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2dceb94-2975-4ce2-b1df-8f9a52fbb138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "762747bc-6a62-4d93-b052-15fb7443cf41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eaaefba-55ae-4a5a-f152-6e88197550ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bibimbap', 'Kimchi', 'Bulgogi', 'Japchae', 'Tteokbokki']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List five {subject}.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "chain.invoke({\"subject\": \"popular Korean cusine\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b861abc3-570c-43f4-8a65-84dbc755b3f2"
      },
      "source": [
        "### 2) JSON Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "f633514f-f8e3-4c16-8fce-2c65fc28a5aa"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "# 자료구조 정의 (pydantic)\n",
        "class CusineRecipe(BaseModel):\n",
        "    name: str = Field(description=\"name of a cusine\")\n",
        "    recipe: str = Field(description=\"recipe to cook the cusine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ce40ce61-056b-4511-8c19-bfff7db60342",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3915a94e-dbbf-4aed-894a-6ba07575f797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of a cusine\", \"type\": \"string\"}, \"recipe\": {\"title\": \"Recipe\", \"description\": \"recipe to cook the cusine\", \"type\": \"string\"}}, \"required\": [\"name\", \"recipe\"]}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# 출력 파서 정의\n",
        "output_parser = JsonOutputParser(pydantic_object=CusineRecipe)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1b14fb66-640e-4ff9-9826-bf4718dcd7a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ec0f5b-b31d-4f55-9765-7ba6283efd71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['query'] partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of a cusine\", \"type\": \"string\"}, \"recipe\": {\"title\": \"Recipe\", \"description\": \"recipe to cook the cusine\", \"type\": \"string\"}}, \"required\": [\"name\", \"recipe\"]}\\n```'} template='Answer the user query.\\n{format_instructions}\\n{query}\\n'\n"
          ]
        }
      ],
      "source": [
        "# prompt 구성\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        ")\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1f42224e-a931-4fb1-868f-d4ed98489744",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5bb63ce-7339-45c1-e74c-54d3355c8fdc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Bibimbap',\n",
              " 'recipe': 'Bibimbap is a traditional Korean dish that consists of mixed rice with vegetables, meat, and a spicy sauce. To cook Bibimbap, you will need the following ingredients: cooked rice, assorted vegetables (such as spinach, bean sprouts, carrots, zucchini, mushrooms), protein (such as beef, chicken, tofu), Gochujang sauce (Korean chili paste), soy sauce, sesame oil'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "chain = prompt | model | output_parser\n",
        "\n",
        "chain.invoke({\"query\": \"Let me know how to cook Bibimbap\"})"
      ]
    }
  ]
}